{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e693e8-9d0e-437d-a876-f6a8a28f4a9b",
   "metadata": {},
   "source": [
    "Primeiro vamos definir a classe Dataset.\n",
    "Ela possui três métodos atualmente: gerarTreinamento, gerarTeste e gerarValidacao.\n",
    "Ambos os métodos retornam um par (textos, notas).\n",
    "O texto textos[i] tirou a nota notas[i]. \n",
    "textos[i] é uma lista de listas: um texto é uma lista de parágrafos.\n",
    "A competencia a ser avaliada é passada como parametro na criação do Dataset --- são 5 competencias (de 0 a 4), cada uma varia de 0 a 5 pontos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77eaf21f-c22b-4bc3-b728-f10779ec4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_dataset import Corpus\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "from skll import metrics as m\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, competence):\n",
    "        self.c = Corpus()\n",
    "        self.c.read_corpus().shape\n",
    "        self.train, self.valid, self.test = self.c.read_splits()\n",
    "        #print(self.train.shape)\n",
    "        self.train.loc[1:5, ['essay', 'score', 'competence']] \n",
    "        #print(self.valid.shape)\n",
    "        self.valid.loc[1:5, ['essay', 'score', 'competence']]\n",
    "        #print(self.test.shape)\n",
    "        self.test.loc[1:5, ['essay', 'score', 'competence']]\n",
    "        self.competence = competence\n",
    "    def UnirListas(self, lista):\n",
    "        unificado = \"\"\n",
    "        unificado = unificado+lista[0]+'\\n'\n",
    "        for t in range(1,len(lista)):\n",
    "            unificado += lista[t]+'\\n'\n",
    "        return unificado\n",
    "        \n",
    "    def gerarTreinamento(self):\n",
    "        \"\"\"\n",
    "        retorna um conjunto (texto, nota_competencia1). Um texto eh composto de varios paragrafos, cada paragrafo eh uma lista \n",
    "        \"\"\"\n",
    "        textos = []\n",
    "        notas = []\n",
    "        for index, row in self.train.iterrows():\n",
    "            texto = self.UnirListas(row['essay'])\n",
    "            notas.append( float(row['competence'][self.competence] / 40))\n",
    "            textos.append(texto)\n",
    "        return textos, notas\n",
    "    def gerarTeste(self):\n",
    "        textos = []\n",
    "        notas = []\n",
    "        for index, row in self.test.iterrows():\n",
    "            texto = self.UnirListas(row['essay'])\n",
    "            notas.append( float(row['competence'][self.competence] / 40))\n",
    "            textos.append(texto)\n",
    "        return textos, notas\n",
    "    def gerarValidacao(self):\n",
    "        textos = []\n",
    "        notas = []\n",
    "        for index, row in self.valid.iterrows():\n",
    "            texto = self.UnirListas(row['essay'])\n",
    "            notas.append( float(row['competence'][self.competence] / 40))\n",
    "            textos.append(texto)\n",
    "        return textos, notas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e96aca-66e6-4d73-9d52-2fb5bd02f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(1)\n",
    "texto_treinamento, nota_treinamento = ds.gerarTreinamento()\n",
    "texto_teste, nota_teste = ds.gerarTeste()\n",
    "texto_valid, nota_valid = ds.gerarValidacao()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee70e7-14bc-4a61-aa77-abf52d50231b",
   "metadata": {},
   "source": [
    "Agora vamos importar o Tokenizer (o mesmo do BERT que usaremos), definiremos para usar a GPU se possível e definiremos a rede neural.\n",
    "\n",
    "A rede neural é: aplicar o BERT sobre a entrada gerando um vetor 1x768, por fim, uma camada densa de 768x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94239f43-7507-49c6-ad68-c18d27f351ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor([7.2027, 8.8833, 1.1151, 0.4434, 0.4453, 2.8503], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "#modelo = 'xlm-roberta-large'\n",
    "modelo = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo,model_max_length=512, truncation=True, do_lower_case=False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "pesos = sklearn.utils.class_weight.compute_class_weight('balanced', classes=[0.0,1.0,2.0,3.0,4.0,5.0], y=nota_treinamento)\n",
    "pesos = torch.tensor(pesos).float().to(\"cuda\")\n",
    "print(pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b149eb6-15be-49b5-91c2-650e808e6204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3198\n",
      "3198\n"
     ]
    }
   ],
   "source": [
    "def TransformarTextoEmInput(textos):\n",
    "    tokenizados = []\n",
    "    for indice in range(len(textos)):\n",
    "            tokens = tokenizer.encode_plus(textos[indice], add_special_tokens=True, truncation=True, return_tensors='pt')\n",
    "            tokens = tokens['input_ids']\n",
    "            tokens = tokens.type(torch.long).to(device)\n",
    "            tokenizados.append(tokens)\n",
    "    return tokenizados\n",
    "\n",
    "def TransformarNotasEmVetor(textos, notas):\n",
    "    novas_notas = []\n",
    "    for indice in range(len(textos)):\n",
    "            novas_notas.append(torch.tensor(int(notas[indice]), dtype=torch.long).unsqueeze(0).to(device))\n",
    "    return novas_notas\n",
    "\n",
    "novos_inputs = TransformarTextoEmInput(texto_treinamento)\n",
    "print(len(novos_inputs))\n",
    "novas_notas = TransformarNotasEmVetor(texto_treinamento, nota_treinamento)\n",
    "print(len(novas_notas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10b6c41-8249-4be0-8098-4ef4da88fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.model = AutoModel.from_pretrained(modelo).to(device)\n",
    "        self.classifier = nn.Sequential(nn.Linear(768,6), nn.Softmax(dim=0))\n",
    "    def forward(self, input_ids):\n",
    "        #print(\"Input: \", input_ids.size())\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids)\n",
    "            #outputs = torch.squeeze(outputs['pooler_output'])\n",
    "            outputs = outputs.last_hidden_state[0][0]\n",
    "        logits = self.classifier(outputs) \n",
    "        return logits\n",
    "model2 = CustomModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa54f48-afdd-4df1-bbf8-9e53a59030ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar(model, inputs, target):\n",
    "    #loss_fn = torch.nn.MSELoss()\n",
    "    #print(pesos)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=pesos)\n",
    "    #loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    vetor_loss = []\n",
    "    optimizer.zero_grad()\n",
    "    count = 0\n",
    "    for index in range(len(inputs)):\n",
    "        count += 1\n",
    "        output = model(inputs[index])\n",
    "        output = output.unsqueeze(0)\n",
    "        #r = torch.tensor(target[index]).unsqueeze(0).long()\n",
    "        #print(target[index])\n",
    "        #r = torch.tensor(target[index], dtype=torch.long)\n",
    "        r = target[index]\n",
    "        #print(r)\n",
    "        #print(output, r)\n",
    "        loss = loss_fn(output, r)\n",
    "        vetor_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        if (count == 1):    \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            count = 0\n",
    "    loss_tmp = sum(vetor_loss)/len(vetor_loss)\n",
    "    print(\"Loss media: \", loss_tmp)\n",
    "    \n",
    "def TokenizarUmParagrafo(paragrafo):\n",
    "    tokens = tokenizer.encode_plus(paragrafo, add_special_tokens=True, truncation=True, return_tensors='pt')\n",
    "    tokens = tokens['input_ids']\n",
    "    tokens = tokens.type(torch.int64).to(device)\n",
    "    return tokens\n",
    "\n",
    "def acuracia_classe(respostas, gold_labels):\n",
    "    classes_chutadas = [0]*6\n",
    "    classes_acertadas = [0]*6\n",
    "    classes_certas = [0]*6\n",
    "    porcentagem_final = []\n",
    "    for r, gl in zip(respostas, gold_labels):\n",
    "        #print(gl)\n",
    "        classes_certas[int(gl)] += 1\n",
    "        classes_chutadas[int(r)] += 1\n",
    "        if (r == gl):\n",
    "            classes_acertadas[int(r)] += 1\n",
    "    for acertos, certo in zip(classes_acertadas, classes_chutadas):\n",
    "        if certo != 0:\n",
    "            porcentagem_final.append( float(acertos) / certo )\n",
    "        else:\n",
    "            porcentagem_final.append(0.0)       \n",
    "    print(\"Chutei: \", classes_chutadas, sum(classes_chutadas))\n",
    "    print(\"Acertei: \", classes_acertadas, sum(classes_acertadas))\n",
    "    print(\"Dist: \", classes_certas, sum(classes_certas))\n",
    "    return porcentagem_final\n",
    "\n",
    "def testar(model, inputs, target):\n",
    "    respostas = []\n",
    "    for index in range(len(inputs)):\n",
    "        #notas_parciais = []\n",
    "        with torch.no_grad():\n",
    "            tokenizado = TokenizarUmParagrafo(inputs[index])\n",
    "            output = model(tokenizado)\n",
    "            nota = output.cpu().detach().numpy()\n",
    "            #notas_parciais.append(nota)\n",
    "            #print(len(notas_parciais))\n",
    "            #nota_final = np.rint(np.average(notas_parciais))\n",
    "            nota_final = np.argmax(nota)\n",
    "        respostas.append(nota_final)\n",
    "    #print(respostas)\n",
    "    print(\"QWK: \", metrics.cohen_kappa_score(target, respostas, weights='quadratic'))\n",
    "    #print(\"RMSE: \", metrics.mean_squared_error(target, respostas, squared=False))\n",
    "    print(\"MSE: \", metrics.mean_squared_error(target, respostas, squared=True))\n",
    "    print(\"Porcentagem das classes: \", acuracia_classe(respostas, target))\n",
    "    print(\"Total acc: \", metrics.accuracy_score(target, respostas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50ba2a3-1e9c-4fa6-93aa-4f8f022b1d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracao  1\n",
      "Loss media:  1.540511811726387\n",
      "-- Treinamento: \n",
      "QWK:  0.33102979110387853\n",
      "MSE:  0.8136335209505942\n",
      "Chutei:  [0, 0, 0, 2141, 1057, 0] 3198\n",
      "Acertei:  [0, 0, 0, 1012, 701, 0] 1713\n",
      "Dist:  [74, 60, 478, 1202, 1197, 187] 3198\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.47267631947687994, 0.663197729422895, 0.0]\n",
      "Total acc:  0.5356472795497186\n",
      "--Validacao:\n",
      "QWK:  0.33777883387293206\n",
      "MSE:  0.8527696793002916\n",
      "Chutei:  [0, 0, 0, 438, 248, 0] 686\n",
      "Acertei:  [0, 0, 0, 210, 146, 0] 356\n",
      "Dist:  [20, 13, 101, 269, 244, 39] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4794520547945205, 0.5887096774193549, 0.0]\n",
      "Total acc:  0.5189504373177842\n",
      "--Teste:\n",
      "QWK:  0.32476294228297\n",
      "MSE:  0.8192419825072886\n",
      "Chutei:  [0, 0, 0, 459, 227, 0] 686\n",
      "Acertei:  [0, 0, 0, 212, 149, 0] 361\n",
      "Dist:  [15, 6, 110, 240, 264, 51] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.46187363834422657, 0.6563876651982379, 0.0]\n",
      "Total acc:  0.5262390670553936\n",
      "Iteracao  2\n",
      "Loss media:  1.5086961248429438\n",
      "-- Treinamento: \n",
      "QWK:  0.35736687244312804\n",
      "MSE:  0.797373358348968\n",
      "Chutei:  [0, 0, 0, 1993, 1205, 0] 3198\n",
      "Acertei:  [0, 0, 0, 977, 791, 0] 1768\n",
      "Dist:  [74, 60, 478, 1202, 1197, 187] 3198\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4902157551430005, 0.6564315352697095, 0.0]\n",
      "Total acc:  0.5528455284552846\n",
      "--Validacao:\n",
      "QWK:  0.33892585660012475\n",
      "MSE:  0.8600583090379009\n",
      "Chutei:  [0, 0, 0, 423, 263, 0] 686\n",
      "Acertei:  [0, 0, 0, 204, 153, 0] 357\n",
      "Dist:  [20, 13, 101, 269, 244, 39] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.48226950354609927, 0.5817490494296578, 0.0]\n",
      "Total acc:  0.5204081632653061\n",
      "--Teste:\n",
      "QWK:  0.34710294335310254\n",
      "MSE:  0.8061224489795918\n",
      "Chutei:  [0, 0, 0, 424, 262, 0] 686\n",
      "Acertei:  [0, 0, 0, 203, 170, 0] 373\n",
      "Dist:  [15, 6, 110, 240, 264, 51] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.47877358490566035, 0.648854961832061, 0.0]\n",
      "Total acc:  0.543731778425656\n",
      "Iteracao  3\n",
      "Loss media:  1.499611419092647\n",
      "-- Treinamento: \n",
      "QWK:  0.3598544231064359\n",
      "MSE:  0.792057535959975\n",
      "Chutei:  [0, 0, 0, 2014, 1184, 0] 3198\n",
      "Acertei:  [0, 0, 0, 989, 786, 0] 1775\n",
      "Dist:  [74, 60, 478, 1202, 1197, 187] 3198\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4910625620655412, 0.6638513513513513, 0.0]\n",
      "Total acc:  0.5550343964978112\n",
      "--Validacao:\n",
      "QWK:  0.35149015233540615\n",
      "MSE:  0.8425655976676385\n",
      "Chutei:  [0, 0, 0, 425, 261, 0] 686\n",
      "Acertei:  [0, 0, 0, 203, 152, 0] 355\n",
      "Dist:  [20, 13, 101, 269, 244, 39] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4776470588235294, 0.5823754789272031, 0.0]\n",
      "Total acc:  0.5174927113702624\n",
      "--Teste:\n",
      "QWK:  0.3348286225970415\n",
      "MSE:  0.8192419825072886\n",
      "Chutei:  [0, 0, 0, 429, 257, 0] 686\n",
      "Acertei:  [0, 0, 0, 203, 164, 0] 367\n",
      "Dist:  [15, 6, 110, 240, 264, 51] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4731934731934732, 0.6381322957198443, 0.0]\n",
      "Total acc:  0.5349854227405247\n",
      "Iteracao  4\n",
      "Loss media:  1.4941783305851648\n",
      "-- Treinamento: \n",
      "QWK:  0.3585835697568305\n",
      "MSE:  0.7879924953095685\n",
      "Chutei:  [0, 0, 0, 2067, 1131, 0] 3198\n",
      "Acertei:  [0, 0, 0, 1008, 764, 0] 1772\n",
      "Dist:  [74, 60, 478, 1202, 1197, 187] 3198\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4876632801161103, 0.6755083996463307, 0.0]\n",
      "Total acc:  0.5540963101938712\n",
      "--Validacao:\n",
      "QWK:  0.3436697544838345\n",
      "MSE:  0.8440233236151603\n",
      "Chutei:  [0, 0, 0, 440, 246, 0] 686\n",
      "Acertei:  [0, 0, 0, 210, 147, 0] 357\n",
      "Dist:  [20, 13, 101, 269, 244, 39] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4772727272727273, 0.5975609756097561, 0.0]\n",
      "Total acc:  0.5204081632653061\n",
      "--Teste:\n",
      "QWK:  0.33712332807105816\n",
      "MSE:  0.8119533527696793\n",
      "Chutei:  [0, 0, 0, 440, 246, 0] 686\n",
      "Acertei:  [0, 0, 0, 204, 159, 0] 363\n",
      "Dist:  [15, 6, 110, 240, 264, 51] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.4636363636363636, 0.6463414634146342, 0.0]\n",
      "Total acc:  0.5291545189504373\n",
      "Iteracao  5\n",
      "Loss media:  1.4886669196360853\n",
      "-- Treinamento: \n",
      "QWK:  0.34944226251825106\n",
      "MSE:  0.7945590994371482\n",
      "Chutei:  [0, 0, 0, 2115, 1082, 1] 3198\n",
      "Acertei:  [0, 0, 0, 1021, 739, 1] 1761\n",
      "Dist:  [74, 60, 478, 1202, 1197, 187] 3198\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.48274231678486995, 0.6829944547134935, 1.0]\n",
      "Total acc:  0.550656660412758\n",
      "--Validacao:\n",
      "QWK:  0.34528706582644697\n",
      "MSE:  0.8367346938775511\n",
      "Chutei:  [0, 0, 0, 449, 237, 0] 686\n",
      "Acertei:  [0, 0, 0, 215, 144, 0] 359\n",
      "Dist:  [20, 13, 101, 269, 244, 39] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.47884187082405344, 0.6075949367088608, 0.0]\n",
      "Total acc:  0.5233236151603499\n",
      "--Teste:\n",
      "QWK:  0.34254385812336285\n",
      "MSE:  0.8032069970845481\n",
      "Chutei:  [0, 0, 1, 448, 237, 0] 686\n",
      "Acertei:  [0, 0, 0, 209, 157, 0] 366\n",
      "Dist:  [15, 6, 110, 240, 264, 51] 686\n",
      "Porcentagem das classes:  [0.0, 0.0, 0.0, 0.46651785714285715, 0.6624472573839663, 0.0]\n",
      "Total acc:  0.5335276967930029\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Iteracao \", i+1)\n",
    "    treinar(model2, novos_inputs, novas_notas)\n",
    "    print(\"-- Treinamento: \")\n",
    "    testar(model2, texto_treinamento, nota_treinamento)\n",
    "    print(\"--Validacao:\")\n",
    "    testar(model2, texto_valid, nota_valid)\n",
    "    print(\"--Teste:\")\n",
    "    testar(model2, texto_teste, nota_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f32ee-a205-4838-a48e-9e9622ae377a",
   "metadata": {},
   "source": [
    "Aqui farei o baseline mais simples de todos: dizer o índice mais comum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58564632-1d7f-4c43-aac9-744ce1dae9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_most_common_index(novas_notas):\n",
    "    indices = [0,0,0,0,0,0]\n",
    "    for i in novas_notas:\n",
    "        indices[int(i)] += 1\n",
    "    maior_valor = -1\n",
    "    maior_indice = -1\n",
    "    print(indices)\n",
    "    for i in range(6):\n",
    "        if(indices[int(i)] > maior_valor):\n",
    "            maior_valor = indices[int(i)]\n",
    "            maior_indice = i\n",
    "    return maior_indice\n",
    "\n",
    "def test_most_common_index(index, notas):\n",
    "    acertos = 0\n",
    "    respostas = []\n",
    "    for i in notas:\n",
    "        respostas.append(index)\n",
    "    print(\"QWK1: \", m.kappa(notas, respostas, weights='quadratic'))\n",
    "    print(\"QWK2: \", metrics.cohen_kappa_score(notas, respostas, weights='quadratic'))\n",
    "    print(\"RMSE: \", metrics.mean_squared_error(notas, respostas, squared=False))\n",
    "    print(\"Acc: \", metrics.accuracy_score(notas, respostas))\n",
    "    #return (acertos/len(notas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f22ab8-54a1-4c1f-b7f9-7de02a589e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 60, 478, 1202, 1197, 187]\n",
      "Treinamento:\n",
      "QWK1:  0.0\n",
      "QWK2:  0.0\n",
      "RMSE:  1.0202759930228251\n",
      "Acc:  0.3758599124452783\n",
      "Teste:\n",
      "QWK1:  0.0\n",
      "QWK2:  0.0\n",
      "RMSE:  1.036505679349426\n",
      "Acc:  0.3498542274052478\n",
      "Validacao:\n",
      "QWK1:  0.0\n",
      "QWK2:  0.0\n",
      "RMSE:  1.0336890826227816\n",
      "Acc:  0.39212827988338195\n"
     ]
    }
   ],
   "source": [
    "indice = learn_most_common_index(nota_treinamento)\n",
    "print(\"Treinamento:\")\n",
    "test_most_common_index(indice, nota_treinamento)\n",
    "print(\"Teste:\")\n",
    "test_most_common_index(indice, nota_teste)\n",
    "print(\"Validacao:\")\n",
    "test_most_common_index(indice, nota_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c9a51-10e1-49a6-93c1-0e826add8410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc002a-85ba-4e12-b28d-0d58d5e9fffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8e8f1-1fad-40d2-9aba-55a2621ecee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fafd9b-71b6-4bcc-be2c-835bd4eb03bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c980b92-eeb6-4bcf-a791-bf596df4f474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b11a5c-024a-4225-8c70-5bcc9eb50e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
